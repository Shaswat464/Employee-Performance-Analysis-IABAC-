{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA SCIENCE PROJECT ON INX FUTURE INC EMPLOYEE PERFORMANCE ANALYSIS**\n",
    "### **BUISNESS CASE: BASED ON GIVEN FEATURE OF DATASET WE NEED TO PREDICT THE PERFOMANCE RATING OF EMPLOYEE**\n",
    "##### MODEL CREATION & EVALUATION SUMMARY:\n",
    "* Loading pre-process data\n",
    "* Define dependant & independant features\n",
    "* Balancing the target feature\n",
    "* Split training and testing data\n",
    "* Model creation, prediction & evaluation\n",
    "* Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import warnings # Used to supressed the warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>pca11</th>\n",
       "      <th>pca12</th>\n",
       "      <th>pca13</th>\n",
       "      <th>pca14</th>\n",
       "      <th>pca15</th>\n",
       "      <th>pca16</th>\n",
       "      <th>pca17</th>\n",
       "      <th>pca18</th>\n",
       "      <th>pca19</th>\n",
       "      <th>pca20</th>\n",
       "      <th>pca21</th>\n",
       "      <th>pca22</th>\n",
       "      <th>pca23</th>\n",
       "      <th>pca24</th>\n",
       "      <th>pca25</th>\n",
       "      <th>PerformanceRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.474617</td>\n",
       "      <td>-1.635606</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.965490</td>\n",
       "      <td>-1.550080</td>\n",
       "      <td>0.179717</td>\n",
       "      <td>0.843739</td>\n",
       "      <td>-1.594458</td>\n",
       "      <td>0.749029</td>\n",
       "      <td>-0.020451</td>\n",
       "      <td>-1.286037</td>\n",
       "      <td>-0.058260</td>\n",
       "      <td>0.407345</td>\n",
       "      <td>-0.249001</td>\n",
       "      <td>-0.004244</td>\n",
       "      <td>0.707790</td>\n",
       "      <td>0.106931</td>\n",
       "      <td>-0.601961</td>\n",
       "      <td>0.097510</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>-0.287547</td>\n",
       "      <td>-0.451346</td>\n",
       "      <td>0.310696</td>\n",
       "      <td>-0.273623</td>\n",
       "      <td>-0.162068</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.357388</td>\n",
       "      <td>-0.057871</td>\n",
       "      <td>2.039849</td>\n",
       "      <td>1.537299</td>\n",
       "      <td>0.291088</td>\n",
       "      <td>1.628643</td>\n",
       "      <td>0.763073</td>\n",
       "      <td>0.155870</td>\n",
       "      <td>1.045583</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>-1.703516</td>\n",
       "      <td>1.157972</td>\n",
       "      <td>-0.339029</td>\n",
       "      <td>0.286234</td>\n",
       "      <td>-0.145409</td>\n",
       "      <td>0.500531</td>\n",
       "      <td>-0.358400</td>\n",
       "      <td>0.423352</td>\n",
       "      <td>-0.879996</td>\n",
       "      <td>-0.539284</td>\n",
       "      <td>-0.275408</td>\n",
       "      <td>-0.922157</td>\n",
       "      <td>-0.175002</td>\n",
       "      <td>-0.684618</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.244991</td>\n",
       "      <td>2.581279</td>\n",
       "      <td>4.424786</td>\n",
       "      <td>-0.162870</td>\n",
       "      <td>-1.914806</td>\n",
       "      <td>1.102650</td>\n",
       "      <td>-1.479360</td>\n",
       "      <td>0.442567</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>1.505285</td>\n",
       "      <td>0.185119</td>\n",
       "      <td>2.371751</td>\n",
       "      <td>0.827040</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>-0.736234</td>\n",
       "      <td>-0.795708</td>\n",
       "      <td>0.507253</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.188903</td>\n",
       "      <td>-0.380440</td>\n",
       "      <td>0.173953</td>\n",
       "      <td>-0.417505</td>\n",
       "      <td>-0.247450</td>\n",
       "      <td>0.745477</td>\n",
       "      <td>-0.371272</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.012637</td>\n",
       "      <td>0.735434</td>\n",
       "      <td>2.433771</td>\n",
       "      <td>3.347248</td>\n",
       "      <td>1.326405</td>\n",
       "      <td>-2.357479</td>\n",
       "      <td>1.226972</td>\n",
       "      <td>0.340809</td>\n",
       "      <td>-0.223106</td>\n",
       "      <td>-0.053884</td>\n",
       "      <td>-0.142110</td>\n",
       "      <td>-1.405722</td>\n",
       "      <td>0.581401</td>\n",
       "      <td>1.134294</td>\n",
       "      <td>1.758583</td>\n",
       "      <td>-0.218867</td>\n",
       "      <td>0.891003</td>\n",
       "      <td>-1.504801</td>\n",
       "      <td>0.590889</td>\n",
       "      <td>0.202439</td>\n",
       "      <td>0.223840</td>\n",
       "      <td>-0.577573</td>\n",
       "      <td>-0.024454</td>\n",
       "      <td>-0.471628</td>\n",
       "      <td>-0.471033</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.249783</td>\n",
       "      <td>5.975149</td>\n",
       "      <td>-0.464801</td>\n",
       "      <td>0.783218</td>\n",
       "      <td>2.877106</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>-0.434443</td>\n",
       "      <td>-0.391564</td>\n",
       "      <td>0.845524</td>\n",
       "      <td>1.203842</td>\n",
       "      <td>-1.614499</td>\n",
       "      <td>0.128538</td>\n",
       "      <td>1.115261</td>\n",
       "      <td>-0.095232</td>\n",
       "      <td>-0.130931</td>\n",
       "      <td>0.812941</td>\n",
       "      <td>-0.300831</td>\n",
       "      <td>1.104789</td>\n",
       "      <td>-1.216270</td>\n",
       "      <td>0.843609</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>-0.177141</td>\n",
       "      <td>0.471097</td>\n",
       "      <td>-0.151107</td>\n",
       "      <td>-0.447970</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0 -4.474617 -1.635606  1.359876  0.965490 -1.550080  0.179717  0.843739   \n",
       "1 -4.357388 -0.057871  2.039849  1.537299  0.291088  1.628643  0.763073   \n",
       "2 -4.244991  2.581279  4.424786 -0.162870 -1.914806  1.102650 -1.479360   \n",
       "3  3.012637  0.735434  2.433771  3.347248  1.326405 -2.357479  1.226972   \n",
       "4 -4.249783  5.975149 -0.464801  0.783218  2.877106  0.052133 -0.434443   \n",
       "\n",
       "       pca8      pca9     pca10     pca11     pca12     pca13     pca14  \\\n",
       "0 -1.594458  0.749029 -0.020451 -1.286037 -0.058260  0.407345 -0.249001   \n",
       "1  0.155870  1.045583  0.797368 -1.703516  1.157972 -0.339029  0.286234   \n",
       "2  0.442567  0.838079  1.505285  0.185119  2.371751  0.827040  0.131010   \n",
       "3  0.340809 -0.223106 -0.053884 -0.142110 -1.405722  0.581401  1.134294   \n",
       "4 -0.391564  0.845524  1.203842 -1.614499  0.128538  1.115261 -0.095232   \n",
       "\n",
       "      pca15     pca16     pca17     pca18     pca19     pca20     pca21  \\\n",
       "0 -0.004244  0.707790  0.106931 -0.601961  0.097510  0.039537 -0.287547   \n",
       "1 -0.145409  0.500531 -0.358400  0.423352 -0.879996 -0.539284 -0.275408   \n",
       "2 -0.736234 -0.795708  0.507253  0.461541  0.188903 -0.380440  0.173953   \n",
       "3  1.758583 -0.218867  0.891003 -1.504801  0.590889  0.202439  0.223840   \n",
       "4 -0.130931  0.812941 -0.300831  1.104789 -1.216270  0.843609  0.101158   \n",
       "\n",
       "      pca22     pca23     pca24     pca25  PerformanceRating  \n",
       "0 -0.451346  0.310696 -0.273623 -0.162068                  3  \n",
       "1 -0.922157 -0.175002 -0.684618 -0.000295                  3  \n",
       "2 -0.417505 -0.247450  0.745477 -0.371272                  4  \n",
       "3 -0.577573 -0.024454 -0.471628 -0.471033                  3  \n",
       "4 -0.177141  0.471097 -0.151107 -0.447970                  3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('employee_performance_analysis_preprocessed_data.csv')\n",
    "pd.set_option('display.max_columns',None) # Used to display the all features\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True) # Drop unwanted feature\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINE INDEPENDANT & DEPENDANT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.PerformanceRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>pca11</th>\n",
       "      <th>pca12</th>\n",
       "      <th>pca13</th>\n",
       "      <th>pca14</th>\n",
       "      <th>pca15</th>\n",
       "      <th>pca16</th>\n",
       "      <th>pca17</th>\n",
       "      <th>pca18</th>\n",
       "      <th>pca19</th>\n",
       "      <th>pca20</th>\n",
       "      <th>pca21</th>\n",
       "      <th>pca22</th>\n",
       "      <th>pca23</th>\n",
       "      <th>pca24</th>\n",
       "      <th>pca25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.474617</td>\n",
       "      <td>-1.635606</td>\n",
       "      <td>1.359876</td>\n",
       "      <td>0.965490</td>\n",
       "      <td>-1.550080</td>\n",
       "      <td>0.179717</td>\n",
       "      <td>0.843739</td>\n",
       "      <td>-1.594458</td>\n",
       "      <td>0.749029</td>\n",
       "      <td>-0.020451</td>\n",
       "      <td>-1.286037</td>\n",
       "      <td>-0.058260</td>\n",
       "      <td>0.407345</td>\n",
       "      <td>-0.249001</td>\n",
       "      <td>-0.004244</td>\n",
       "      <td>0.707790</td>\n",
       "      <td>0.106931</td>\n",
       "      <td>-0.601961</td>\n",
       "      <td>0.097510</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>-0.287547</td>\n",
       "      <td>-0.451346</td>\n",
       "      <td>0.310696</td>\n",
       "      <td>-0.273623</td>\n",
       "      <td>-0.162068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.357388</td>\n",
       "      <td>-0.057871</td>\n",
       "      <td>2.039849</td>\n",
       "      <td>1.537299</td>\n",
       "      <td>0.291088</td>\n",
       "      <td>1.628643</td>\n",
       "      <td>0.763073</td>\n",
       "      <td>0.155870</td>\n",
       "      <td>1.045583</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>-1.703516</td>\n",
       "      <td>1.157972</td>\n",
       "      <td>-0.339029</td>\n",
       "      <td>0.286234</td>\n",
       "      <td>-0.145409</td>\n",
       "      <td>0.500531</td>\n",
       "      <td>-0.358400</td>\n",
       "      <td>0.423352</td>\n",
       "      <td>-0.879996</td>\n",
       "      <td>-0.539284</td>\n",
       "      <td>-0.275408</td>\n",
       "      <td>-0.922157</td>\n",
       "      <td>-0.175002</td>\n",
       "      <td>-0.684618</td>\n",
       "      <td>-0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.244991</td>\n",
       "      <td>2.581279</td>\n",
       "      <td>4.424786</td>\n",
       "      <td>-0.162870</td>\n",
       "      <td>-1.914806</td>\n",
       "      <td>1.102650</td>\n",
       "      <td>-1.479360</td>\n",
       "      <td>0.442567</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>1.505285</td>\n",
       "      <td>0.185119</td>\n",
       "      <td>2.371751</td>\n",
       "      <td>0.827040</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>-0.736234</td>\n",
       "      <td>-0.795708</td>\n",
       "      <td>0.507253</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.188903</td>\n",
       "      <td>-0.380440</td>\n",
       "      <td>0.173953</td>\n",
       "      <td>-0.417505</td>\n",
       "      <td>-0.247450</td>\n",
       "      <td>0.745477</td>\n",
       "      <td>-0.371272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.012637</td>\n",
       "      <td>0.735434</td>\n",
       "      <td>2.433771</td>\n",
       "      <td>3.347248</td>\n",
       "      <td>1.326405</td>\n",
       "      <td>-2.357479</td>\n",
       "      <td>1.226972</td>\n",
       "      <td>0.340809</td>\n",
       "      <td>-0.223106</td>\n",
       "      <td>-0.053884</td>\n",
       "      <td>-0.142110</td>\n",
       "      <td>-1.405722</td>\n",
       "      <td>0.581401</td>\n",
       "      <td>1.134294</td>\n",
       "      <td>1.758583</td>\n",
       "      <td>-0.218867</td>\n",
       "      <td>0.891003</td>\n",
       "      <td>-1.504801</td>\n",
       "      <td>0.590889</td>\n",
       "      <td>0.202439</td>\n",
       "      <td>0.223840</td>\n",
       "      <td>-0.577573</td>\n",
       "      <td>-0.024454</td>\n",
       "      <td>-0.471628</td>\n",
       "      <td>-0.471033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.249783</td>\n",
       "      <td>5.975149</td>\n",
       "      <td>-0.464801</td>\n",
       "      <td>0.783218</td>\n",
       "      <td>2.877106</td>\n",
       "      <td>0.052133</td>\n",
       "      <td>-0.434443</td>\n",
       "      <td>-0.391564</td>\n",
       "      <td>0.845524</td>\n",
       "      <td>1.203842</td>\n",
       "      <td>-1.614499</td>\n",
       "      <td>0.128538</td>\n",
       "      <td>1.115261</td>\n",
       "      <td>-0.095232</td>\n",
       "      <td>-0.130931</td>\n",
       "      <td>0.812941</td>\n",
       "      <td>-0.300831</td>\n",
       "      <td>1.104789</td>\n",
       "      <td>-1.216270</td>\n",
       "      <td>0.843609</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>-0.177141</td>\n",
       "      <td>0.471097</td>\n",
       "      <td>-0.151107</td>\n",
       "      <td>-0.447970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0 -4.474617 -1.635606  1.359876  0.965490 -1.550080  0.179717  0.843739   \n",
       "1 -4.357388 -0.057871  2.039849  1.537299  0.291088  1.628643  0.763073   \n",
       "2 -4.244991  2.581279  4.424786 -0.162870 -1.914806  1.102650 -1.479360   \n",
       "3  3.012637  0.735434  2.433771  3.347248  1.326405 -2.357479  1.226972   \n",
       "4 -4.249783  5.975149 -0.464801  0.783218  2.877106  0.052133 -0.434443   \n",
       "\n",
       "       pca8      pca9     pca10     pca11     pca12     pca13     pca14  \\\n",
       "0 -1.594458  0.749029 -0.020451 -1.286037 -0.058260  0.407345 -0.249001   \n",
       "1  0.155870  1.045583  0.797368 -1.703516  1.157972 -0.339029  0.286234   \n",
       "2  0.442567  0.838079  1.505285  0.185119  2.371751  0.827040  0.131010   \n",
       "3  0.340809 -0.223106 -0.053884 -0.142110 -1.405722  0.581401  1.134294   \n",
       "4 -0.391564  0.845524  1.203842 -1.614499  0.128538  1.115261 -0.095232   \n",
       "\n",
       "      pca15     pca16     pca17     pca18     pca19     pca20     pca21  \\\n",
       "0 -0.004244  0.707790  0.106931 -0.601961  0.097510  0.039537 -0.287547   \n",
       "1 -0.145409  0.500531 -0.358400  0.423352 -0.879996 -0.539284 -0.275408   \n",
       "2 -0.736234 -0.795708  0.507253  0.461541  0.188903 -0.380440  0.173953   \n",
       "3  1.758583 -0.218867  0.891003 -1.504801  0.590889  0.202439  0.223840   \n",
       "4 -0.130931  0.812941 -0.300831  1.104789 -1.216270  0.843609  0.101158   \n",
       "\n",
       "      pca22     pca23     pca24     pca25  \n",
       "0 -0.451346  0.310696 -0.273623 -0.162068  \n",
       "1 -0.922157 -0.175002 -0.684618 -0.000295  \n",
       "2 -0.417505 -0.247450  0.745477 -0.371272  \n",
       "3 -0.577573 -0.024454 -0.471628 -0.471033  \n",
       "4 -0.177141  0.471097 -0.151107 -0.447970  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    3\n",
       "2    4\n",
       "3    3\n",
       "4    3\n",
       "Name: PerformanceRating, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BALANCING THE TARGET FEATURE\n",
    "* SMOTE: SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesises new minority instances between existing minority instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbalanced data   :   Counter({3: 874, 2: 194, 4: 132})\n",
      "balanced data:    : Counter({3: 874, 4: 874, 2: 874})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE #SMOTE(synthetic minority oversampling techinque)\n",
    "sm = SMOTE() # obeject creation\n",
    "print(\"unbalanced data   :  \",Counter(y))\n",
    "X_sm,y_sm = sm.fit_resample(X,y)\n",
    "print(\"balanced data:    :\",Counter(y_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now target feature in balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT TRAINING AND TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_sm,y_sm,random_state=42,test_size=0.20) # 20% data given to testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2097, 25), (525, 25), (2097,), (525,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of train and test\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL CREATION, PREDICTION AND EVALUATION\n",
    " AIM\n",
    "* Create a sweet spot model (Low bias, Low variance)\n",
    "##### HERE WE WILL BE EXPERIMENTING WITH THREE ALGORITHM\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* XGBOOST\n",
    "* Decision tree\n",
    "* Artificial Neural Network [MLP Classifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy: 0.9995\n",
      "SVM Test Accuracy: 0.9981\n",
      "Support Vector Machine (SVM) Performance on Test Data:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      1.00      1.00       184\n",
      "           3       1.00      0.99      1.00       173\n",
      "           4       1.00      1.00      1.00       168\n",
      "\n",
      "    accuracy                           1.00       525\n",
      "   macro avg       1.00      1.00      1.00       525\n",
      "weighted avg       1.00      1.00      1.00       525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[184   0   0]\n",
      " [  1 172   0]\n",
      " [  0   0 168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_svm = svm_model.predict(X_train)\n",
    "y_test_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "print(f\"SVM Training Accuracy: {train_accuracy_svm:.4f}\")\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "print(f\"SVM Test Accuracy: {test_accuracy_svm:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix for test set\n",
    "print(\"Support Vector Machine (SVM) Performance on Test Data:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_svm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 1.0000\n",
      "Random Forest Test Accuracy: 0.9886\n",
      "Random Forest Performance on Test Data:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.98      0.99      0.98       184\n",
      "           3       0.99      0.98      0.98       173\n",
      "           4       1.00      1.00      1.00       168\n",
      "\n",
      "    accuracy                           0.99       525\n",
      "   macro avg       0.99      0.99      0.99       525\n",
      "weighted avg       0.99      0.99      0.99       525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[182   2   0]\n",
      " [  4 169   0]\n",
      " [  0   0 168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"Random Forest Training Accuracy: {train_accuracy_rf:.4f}\")\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "print(f\"Random Forest Test Accuracy: {test_accuracy_rf:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix for test set\n",
    "print(\"Random Forest Performance on Test Data:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DecisionTree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Training Accuracy: 1.0000\n",
      "Decision Tree Test Accuracy: 0.9276\n",
      "Decision Tree Performance on Test Data:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.94      0.97      0.96       184\n",
      "           3       0.92      0.86      0.89       173\n",
      "           4       0.92      0.95      0.94       168\n",
      "\n",
      "    accuracy                           0.93       525\n",
      "   macro avg       0.93      0.93      0.93       525\n",
      "weighted avg       0.93      0.93      0.93       525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   5   0]\n",
      " [ 11 148  14]\n",
      " [  0   8 160]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_dt = dt_model.predict(X_train)\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "print(f\"Decision Tree Training Accuracy: {train_accuracy_dt:.4f}\")\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "print(f\"Decision Tree Test Accuracy: {test_accuracy_dt:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix for test set\n",
    "print(\"Decision Tree Performance on Test Data:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_dt))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Training Accuracy: 1.0000\n",
      "XGBoost Test Accuracy: 0.9810\n",
      "XGBoost Performance on Test Data:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       184\n",
      "           1       0.99      0.95      0.97       173\n",
      "           2       0.99      1.00      0.99       168\n",
      "\n",
      "    accuracy                           0.98       525\n",
      "   macro avg       0.98      0.98      0.98       525\n",
      "weighted avg       0.98      0.98      0.98       525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[182   2   0]\n",
      " [  6 165   2]\n",
      " [  0   0 168]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable to have values starting from 0\n",
    "y_encoded = label_encoder.fit_transform(y_sm)\n",
    "\n",
    "# Split the encoded data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_encoded, random_state=42, test_size=0.20)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "print(f\"XGBoost Training Accuracy: {train_accuracy_xgb:.4f}\")\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "print(f\"XGBoost Test Accuracy: {test_accuracy_xgb:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix for test set\n",
    "print(\"XGBoost Performance on Test Data:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_xgb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Training Accuracy: 1.0000\n",
      "MLP Classifier Test Accuracy: 0.9962\n",
      "MLP Classifier (Neural Network) Performance on Test Data:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      1.00      1.00       184\n",
      "           3       1.00      0.99      0.99       173\n",
      "           4       0.99      1.00      1.00       168\n",
      "\n",
      "    accuracy                           1.00       525\n",
      "   macro avg       1.00      1.00      1.00       525\n",
      "weighted avg       1.00      1.00      1.00       525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[184   0   0]\n",
      " [  1 171   1]\n",
      " [  0   0 168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize the MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred_mlp = mlp_model.predict(X_train)\n",
    "y_test_pred_mlp = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "train_accuracy_mlp = accuracy_score(y_train, y_train_pred_mlp)\n",
    "print(f\"MLP Classifier Training Accuracy: {train_accuracy_mlp:.4f}\")\n",
    "\n",
    "# Evaluate test accuracy\n",
    "test_accuracy_mlp = accuracy_score(y_test, y_test_pred_mlp)\n",
    "print(f\"MLP Classifier Test Accuracy: {test_accuracy_mlp:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix for test set\n",
    "print(\"MLP Classifier (Neural Network) Performance on Test Data:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_mlp))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for all Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for SVM: {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
      "Best Score for SVM: 1.0\n",
      "SVM Training Accuracy: 1.0\n",
      "SVM Testing Accuracy: 1.0\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       184\n",
      "           1       1.00      1.00      1.00       173\n",
      "           2       1.00      1.00      1.00       168\n",
      "\n",
      "    accuracy                           1.00       525\n",
      "   macro avg       1.00      1.00      1.00       525\n",
      "weighted avg       1.00      1.00      1.00       525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# SVM Hyperparameter Tuning\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "random_search_svm = RandomizedSearchCV(svm_model, svm_param_grid, n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "random_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for SVM:\", random_search_svm.best_params_)\n",
    "print(\"Best Score for SVM:\", random_search_svm.best_score_)\n",
    "\n",
    "# Evaluate on training and test set\n",
    "svm_best = random_search_svm.best_estimator_\n",
    "y_train_pred_svm = svm_best.predict(X_train)\n",
    "y_test_pred_svm = svm_best.predict(X_test)\n",
    "\n",
    "train_accuracy_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "\n",
    "print(\"SVM Training Accuracy:\", train_accuracy_svm)\n",
    "print(\"SVM Testing Accuracy:\", test_accuracy_svm)\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, y_test_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 40, 'bootstrap': False}\n",
      "Best Score for Random Forest: 0.9957086032503695\n",
      "Random Forest Training Accuracy: 1.0\n",
      "Random Forest Testing Accuracy: 0.9942857142857143\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       184\n",
      "           1       0.99      0.99      0.99       173\n",
      "           2       1.00      1.00      1.00       168\n",
      "\n",
      "    accuracy                           0.99       525\n",
      "   macro avg       0.99      0.99      0.99       525\n",
      "weighted avg       0.99      0.99      0.99       525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Hyperparameter Tuning\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "random_search_rf = RandomizedSearchCV(rf_model, rf_param_grid, n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best Score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate on training and test set\n",
    "rf_best = random_search_rf.best_estimator_\n",
    "y_train_pred_rf = rf_best.predict(X_train)\n",
    "y_test_pred_rf = rf_best.predict(X_test)\n",
    "\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"Random Forest Training Accuracy:\", train_accuracy_rf)\n",
    "print(\"Random Forest Testing Accuracy:\", test_accuracy_rf)\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_test_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DecisionTree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for Decision Tree: {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'entropy'}\n",
      "Best Score for Decision Tree: 0.9356154108421411\n",
      "Decision Tree Training Accuracy: 0.9938006676204101\n",
      "Decision Tree Testing Accuracy: 0.9314285714285714\n",
      "Decision Tree Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94       184\n",
      "           1       0.90      0.90      0.90       173\n",
      "           2       0.97      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.93       525\n",
      "   macro avg       0.93      0.93      0.93       525\n",
      "weighted avg       0.93      0.93      0.93       525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Hyperparameter Tuning\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "random_search_dt = RandomizedSearchCV(dt_model, dt_param_grid, n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for Decision Tree:\", random_search_dt.best_params_)\n",
    "print(\"Best Score for Decision Tree:\", random_search_dt.best_score_)\n",
    "\n",
    "# Evaluate on training and test set\n",
    "dt_best = random_search_dt.best_estimator_\n",
    "y_train_pred_dt = dt_best.predict(X_train)\n",
    "y_test_pred_dt = dt_best.predict(X_test)\n",
    "\n",
    "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "print(\"Decision Tree Training Accuracy:\", train_accuracy_dt)\n",
    "print(\"Decision Tree Testing Accuracy:\", test_accuracy_dt)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, y_test_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for XGBoost: {'subsample': 0.6, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.2, 'colsample_bytree': 0.6}\n",
      "Best Score for XGBoost: 0.9961847937265599\n",
      "XGBoost Training Accuracy: 1.0\n",
      "XGBoost Testing Accuracy: 0.9866666666666667\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       184\n",
      "           1       0.99      0.97      0.98       173\n",
      "           2       0.99      1.00      1.00       168\n",
      "\n",
      "    accuracy                           0.99       525\n",
      "   macro avg       0.99      0.99      0.99       525\n",
      "weighted avg       0.99      0.99      0.99       525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Hyperparameter Tuning\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, xgb_param_grid, n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for XGBoost:\", random_search_xgb.best_params_)\n",
    "print(\"Best Score for XGBoost:\", random_search_xgb.best_score_)\n",
    "\n",
    "# Evaluate on training and test set\n",
    "xgb_best = random_search_xgb.best_estimator_\n",
    "y_train_pred_xgb = xgb_best.predict(X_train)\n",
    "y_test_pred_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Training Accuracy:\", train_accuracy_xgb)\n",
    "print(\"XGBoost Testing Accuracy:\", test_accuracy_xgb)\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_test_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for MLP Classifier: {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (100, 50), 'alpha': 0.0001, 'activation': 'tanh'}\n",
      "Best Score for MLP Classifier: 1.0\n",
      "MLP Classifier Training Accuracy: 1.0\n",
      "MLP Classifier Testing Accuracy: 1.0\n",
      "MLP Classifier Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       184\n",
      "           1       1.00      1.00      1.00       173\n",
      "           2       1.00      1.00      1.00       168\n",
      "\n",
      "    accuracy                           1.00       525\n",
      "   macro avg       1.00      1.00      1.00       525\n",
      "weighted avg       1.00      1.00      1.00       525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# MLP Classifier Hyperparameter Tuning\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50), (100,), (100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=1000, random_state=42)\n",
    "random_search_mlp = RandomizedSearchCV(mlp_model, mlp_param_grid, n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "random_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for MLP Classifier:\", random_search_mlp.best_params_)\n",
    "print(\"Best Score for MLP Classifier:\", random_search_mlp.best_score_)\n",
    "\n",
    "# Evaluate on training and test set\n",
    "mlp_best = random_search_mlp.best_estimator_\n",
    "y_train_pred_mlp = mlp_best.predict(X_train)\n",
    "y_test_pred_mlp = mlp_best.predict(X_test)\n",
    "\n",
    "train_accuracy_mlp = accuracy_score(y_train, y_train_pred_mlp)\n",
    "test_accuracy_mlp = accuracy_score(y_test, y_test_pred_mlp)\n",
    "\n",
    "print(\"MLP Classifier Training Accuracy:\", train_accuracy_mlp)\n",
    "print(\"MLP Classifier Testing Accuracy:\", test_accuracy_mlp)\n",
    "print(\"MLP Classifier Classification Report:\\n\", classification_report(y_test, y_test_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model               Train Accuracy Before    Test Accuracy Before     Train Accuracy After Tuning   Test Accuracy After Tuning    \n",
      "===============================================================================================================================\n",
      "SVM                 0.999500                      0.998100                      1.000000                      1.000000                      \n",
      "Random Forest       1.000000                      0.988600                      1.000000                      0.994200                      \n",
      "Decision Tree       1.000000                      0.927600                      0.993800                      0.931400                      \n",
      "XGBoost             1.000000                      0.981000                      1.000000                      0.986600                      \n",
      "MLP Classifier      1.000000                      0.996200                      1.000000                      1.000000                      \n"
     ]
    }
   ],
   "source": [
    "# Accuracies before tuning\n",
    "train_accuracy_svm_before = 0.9995\n",
    "test_accuracy_svm_before = 0.9981\n",
    "\n",
    "train_accuracy_rf_before = 1.0000\n",
    "test_accuracy_rf_before = 0.9886\n",
    "\n",
    "train_accuracy_dt_before = 1.0000\n",
    "test_accuracy_dt_before = 0.9276\n",
    "\n",
    "train_accuracy_xgb_before = 1.0000\n",
    "test_accuracy_xgb_before = 0.9810\n",
    "\n",
    "train_accuracy_mlp_before = 1.0000\n",
    "test_accuracy_mlp_before = 0.9962\n",
    "\n",
    "# Accuracies after tuning\n",
    "train_accuracy_svm_after = 1.0000\n",
    "test_accuracy_svm_after = 1.0000\n",
    "\n",
    "train_accuracy_rf_after = 1.0000\n",
    "test_accuracy_rf_after = 0.9942\n",
    "\n",
    "train_accuracy_dt_after = 0.9938\n",
    "test_accuracy_dt_after = 0.9314\n",
    "\n",
    "train_accuracy_xgb_after = 1.0000\n",
    "test_accuracy_xgb_after = 0.9866\n",
    "\n",
    "train_accuracy_mlp_after = 1.0000\n",
    "test_accuracy_mlp_after = 1.0000\n",
    "\n",
    "# Print results in a tabular format\n",
    "print(f\"{'Model':<20}{'Train Accuracy Before':<25}{'Test Accuracy Before':<25}{'Train Accuracy After Tuning':<30}{'Test Accuracy After Tuning':<30}\")\n",
    "print(\"=\"*127)\n",
    "\n",
    "print(f\"{'SVM':<20}{train_accuracy_svm_before:<30.6f}{test_accuracy_svm_before:<30.6f}{train_accuracy_svm_after:<30.6f}{test_accuracy_svm_after:<30.6f}\")\n",
    "print(f\"{'Random Forest':<20}{train_accuracy_rf_before:<30.6f}{test_accuracy_rf_before:<30.6f}{train_accuracy_rf_after:<30.6f}{test_accuracy_rf_after:<30.6f}\")\n",
    "print(f\"{'Decision Tree':<20}{train_accuracy_dt_before:<30.6f}{test_accuracy_dt_before:<30.6f}{train_accuracy_dt_after:<30.6f}{test_accuracy_dt_after:<30.6f}\")\n",
    "print(f\"{'XGBoost':<20}{train_accuracy_xgb_before:<30.6f}{test_accuracy_xgb_before:<30.6f}{train_accuracy_xgb_after:<30.6f}{test_accuracy_xgb_after:<30.6f}\")\n",
    "print(f\"{'MLP Classifier':<20}{train_accuracy_mlp_before:<30.6f}{test_accuracy_mlp_before:<30.6f}{train_accuracy_mlp_after:<30.6f}{test_accuracy_mlp_after:<30.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a summary of the model performance before and after hyperparameter tuning:\n",
    "##### Summary Before Hyperparameter Tuning:\n",
    "##### SVM:\n",
    "* Training Accuracy: 99.95%\n",
    "* Test Accuracy: 99.81%\n",
    "##### Random Forest:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 98.86%\n",
    "##### Decision Tree:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 92.76%\n",
    "##### XGBoost:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 98.10%\n",
    "##### MLP Classifier:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 99.62%\n",
    "#### Summary After Hyperparameter Tuning:\n",
    "##### SVM:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 100.00%\n",
    "##### Random Forest:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 99.42%\n",
    "##### Decision Tree:\n",
    "* Training Accuracy: 99.38%\n",
    "* Test Accuracy: 93.14%\n",
    "##### XGBoost:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 98.66%\n",
    "##### MLP Classifier:\n",
    "* Training Accuracy: 100.00%\n",
    "* Test Accuracy: 100.00%\n",
    "\n",
    "**Based on these results, the MLP Classifier and SVM show the highest test accuracy after hyperparameter tuning (100.00%). Both models seem to be performing excellently.So I save the MLP model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP model saved successfully as 'best_mlp_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best MLP model\n",
    "with open('best_mlp_model.pkl', 'wb') as file:\n",
    "    pickle.dump(mlp_best, file)\n",
    "\n",
    "print(\"Best MLP model saved successfully as 'best_mlp_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
